{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MDBA Machine Learning Notebook\n",
    "\n",
    "Annotating and training an object detector for automated counting of species\n",
    "\n",
    "IMPORTANT NOTE: Parameters in params.yaml are used throughout the notebook.\n",
    "\n",
    "The paths used throughout the notebook are relative and therefore the working directory must be the root of the machine learning directory. E.g.\n",
    "\n",
    "<ul>\n",
    "<li>Root</li>\n",
    "    <ul>\n",
    "    <li>data</li>    \n",
    "    <li>inference</li>\n",
    "    <li>models</li>\n",
    "    <li>notebooks</li>\n",
    "    <li>report</li>\n",
    "    <li>requirements</li>\n",
    "    <li>scripts</li>\n",
    "    <li>test</li>    \n",
    "    <li>utils</li>\n",
    "    <li>params.yaml</li>\n",
    "    <li>README.md</li>\n",
    "    </ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory here\n",
    "import sys, os\n",
    "os.chdir(r'/home/azureuser/cloudfiles//code/Users/Ahsanul.Habib/WaterbirdCount/Drone-based-waterbird-counting')\n",
    "os.getcwd()\n",
    "!export PYTHONPATH=$rootfolder\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/waterbirds/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import re\n",
    "import csv\n",
    "import cv2\n",
    "import math\n",
    "import codecs, json\n",
    "from json import JSONEncoder\n",
    "from tqdm import tqdm\n",
    "from simple_colors import black\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from random import seed, shuffle, sample\n",
    "from shutil import copyfile, move\n",
    "from subprocess import run, Popen\n",
    "import yaml\n",
    "from yaml import safe_load\n",
    "import pylab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import gc\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "from utils.train_utils import collate_fn, get_transform, BirdDataset\n",
    "from utils.Slicer import Slicer\n",
    "from utils.utils import ensure_path\n",
    "from utils.data_utils import CocoDataset\n",
    "from utils.utils import str2bool\n",
    "from utils.count import count_folder\n",
    "\n",
    "from scripts.prepare_training_set import save_slices, cocofy_annotations, prepare_training_set\n",
    "from scripts.split_raw_dataset import split_dataset, generate_training_set\n",
    "from scripts.train_mdba import train_loop_fn, eval_loop_fn, train_mdba\n",
    "from scripts.eval_mdba import groundTruth_annotations, draw_bboxes_save_results, plot_holdout_images_with_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[3;30mparams.yml\u001b[0m file is loaded.\n"
     ]
    }
   ],
   "source": [
    "with open('./params.yaml', 'r') as params_file:\n",
    "    params = yaml.safe_load(params_file)\n",
    "    print(black('params.yml', ['italic']) + ' file is loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split raw dataset \n",
    "\n",
    "This module takes raw images and a point file created in dotdotgoose to split them into appropriate size slices for training.\n",
    "\n",
    "Slice size, as well as raw image size must be specified in the .yaml read in in the 2nd code chunk.\n",
    "\n",
    "The slices are automatically placed into a new directory to be ingested by the 'prepare_training_set.py' module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering out images with missing annotation data: 100%|██████████| 205/205 [00:00<00:00, 1034695.93it/s]\n",
      "Filtering out empty images: 100%|██████████| 188/188 [02:18<00:00,  1.36it/s]\n",
      "Creating holdouts set: 100%|██████████| 15/15 [00:01<00:00, 12.24it/s]\n",
      "Creating slices dictionary: 100%|██████████| 188/188 [00:00<00:00, 3268.27it/s]\n",
      "Saving trainval slices: 100%|██████████| 85/85 [05:17<00:00,  3.74s/it]\n",
      "Saving holdouts slices: 100%|██████████| 15/15 [02:48<00:00, 11.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialise all required variables\n",
    "raw_dir = Path(params['data']['raw_dir'])\n",
    "project_name = params['data']['project_name']\n",
    "project_folder = raw_dir / project_name\n",
    "points_file = [file for file in os.listdir(project_folder) if file.endswith('.pnt')][0]\n",
    "points_path = project_folder / points_file\n",
    "\n",
    "holdouts_dir = Path(params['data']['holdouts_dir'])/(project_name+'_v1')\n",
    "filenames = sorted(os.listdir(holdouts_dir))\n",
    "no_annotation = [filename for filename in filenames if filename.lower().endswith('.jpg')]\n",
    "\n",
    "trainval_dir = Path(params['data']['trainval_dir'])/project_name\n",
    "holdouts_dir = Path(params['data']['holdouts_dir'])/project_name\n",
    "nolabels_dir = Path(params['data']['nolabels_dir'])/project_name\n",
    "trainval_output_dir = Path(params['slices']['trainval_dir'])/project_name/'sliced_images'\n",
    "holdouts_output_dir = Path(params['slices']['holdouts_dir'])/project_name/'sliced_images'\n",
    "\n",
    "# The .pnt file is necessary to prepare a training set.\n",
    "# Otherwise the number of slices quickly becomes intractable.\n",
    "points_file = [file for file in os.listdir(project_folder) if file.endswith('.pnt')][0]\n",
    "points_path = project_folder / points_file\n",
    "try:\n",
    "    with open(points_path) as file:\n",
    "        data = file.read()\n",
    "        data = json.loads(data)\n",
    "        points_dict = data.get('points')\n",
    "except FileNotFoundError as err:\n",
    "    print(f\"{err}\")\n",
    "    raise\n",
    "except NameError as err:\n",
    "    print(f\"{err}\")\n",
    "    print(f\"Ensure that the .pnt file is properly formatted\")\n",
    "    raise\n",
    "\n",
    "# First split the raw images into categories: trainval, holdouts, nolabels\n",
    "holdouts_set, updated_points_dict = split_dataset(params, raw_dir / project_name, points_dict, trainval_dir, holdouts_dir, nolabels_dir, no_annotation)\n",
    "points_dict = updated_points_dict\n",
    "# Then, for the trainval and holdouts datasets, split them up and save the slices that have been identified by the .pnt file as containing objects. \n",
    "# This is done to filter out empty slices and reduce the labelling required for building a detector.\n",
    "generate_training_set(params, points_dict, holdouts_set, trainval_dir, holdouts_dir, trainval_output_dir, holdouts_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training set\n",
    "\n",
    "This script is used to scan through labels produced from label studio \n",
    "and prepare a dataset in the COCO format required for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding existing annotations: 100%|██████████| 1759/1759 [03:21<00:00,  8.75it/s]\n",
      "Adding negative samples: 100%|██████████| 88/88 [02:03<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "prepare_training_set(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Faster-RCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Epoch --> 1 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/203 [00:00<?, ?it/s]/anaconda/envs/waterbirds/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Training: 100%|██████████| 203/203 [05:32<00:00,  1.64s/it]\n",
      "/anaconda/envs/waterbirds/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.5053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:43<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.4095\n",
      "Epoch --> 2 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:42<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:43<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3930\n",
      "Epoch --> 3 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:42<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:43<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3865\n",
      "Epoch --> 4 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:42<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:43<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3838\n",
      "Epoch --> 5 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:42<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:43<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3856\n",
      "Epoch --> 6 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:42<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:43<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3797\n",
      "Epoch --> 7 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 46/203 [01:18<04:25,  1.69s/it]"
     ]
    }
   ],
   "source": [
    "train_mdba(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate trained model on the holdout set\n",
    "\n",
    "If ground truth bounding boxes are provided, this module performs inference on the holdout image set and draws bounding boxes for ground truth data (green) and predicted data (blue/red).\n",
    "\n",
    "The results json and images with bounding boxes drawn are save in: *./inference/results/DJI_202109281012_017_Mid_Lake_1010_70m_D2_50mm_10-lap_detections_available/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices_holdout_dir = Path(params['slices']['holdouts_dir'])/params['data']['project_name']/'sliced_images'\n",
    "no_annotation_dir = Path(params['data']['holdouts_dir'])/(params['data']['project_name']+'_v1')\n",
    "train_data_dir = Path(params['training']['train_data_dir'])/params['data']['project_name']/'sliced_images'\n",
    "included_filenames = sorted(os.listdir(slices_holdout_dir))\n",
    "included_files = [filename for filename in included_filenames if filename.lower().endswith('.jpg')]\n",
    "labels_path = params['slices']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Obtaining ground truth annotation for holdout set: 100%|██████████| 2201/2201 [00:01<00:00, 2151.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# This function transforms, or re-formats, the labels into the required format for inference.\n",
    "with open(labels_path, 'r') as fr:\n",
    "    ground_truth = groundTruth_annotations(params, slices_holdout_dir, json.loads(fr.read()), included_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_filepaths = [ground_truth[i]['filepath'] for i in range(len(ground_truth))]\n",
    "filenames = [os.path.basename(gt_filepaths[i]) for i in range(len(gt_filepaths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing ground-truth and predicted boxes around birds in holdout set with available annotation data:   0%|          | 0/357 [00:00<?, ?it/s]/anaconda/envs/waterbirds/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Drawing ground-truth and predicted boxes around birds in holdout set with available annotation data: 100%|██████████| 357/357 [02:55<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "groundtruths, inferences, images_with_bbox = draw_bboxes_save_results(params, filenames, ground_truth, slices_holdout_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_holdout_images_with_bboxes(images_with_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate performance metric (mAP) for the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MeanAveragePrecision(box_format='xyxy', iou_type='bbox', iou_thresholds=list(np.linspace(0.5,0.95,91)), max_detection_thresholds=[3000])\n",
    "metric.update(inferences, groundtruths)\n",
    "result = metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@IoU=50%: 0.8480085134506226\n",
      "mAP@IoU=75%: 0.6322376728057861\n"
     ]
    }
   ],
   "source": [
    "print(f\"mAP@IoU=50%: {result['map_50'].item()}\")\n",
    "print(f\"mAP@IoU=75%: {result['map_75'].item()}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8975dbe95a9696821a508f700b23ea164b3f8fcfb057d4e51a45b47dea22a8ca"
  },
  "kernelspec": {
   "display_name": "waterbirds",
   "language": "python",
   "name": "waterbirds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
