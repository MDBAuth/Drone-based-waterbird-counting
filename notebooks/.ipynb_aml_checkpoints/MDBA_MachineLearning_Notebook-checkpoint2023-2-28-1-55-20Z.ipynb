{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDBA Machine Learning Notebook\n",
    "\n",
    "Annotating and training an object detector for automated counting of species\n",
    "\n",
    "IMPORTANT NOTE: Parameters in params.yaml are used throughout the notebook.\n",
    "\n",
    "The paths used throughout the notebook are relative and therefore the working directory must be the root of the machine learning directory. E.g.\n",
    "\n",
    "<ul>\n",
    "<li>Root</li>\n",
    "    <ul>\n",
    "    <li>Data</li>\n",
    "    <li>Models</li>\n",
    "    <li>Scripts</li>\n",
    "    <li>Utils</li>\n",
    "    <li>requirements.txt</li>\n",
    "    <li>params.yaml</li>\n",
    "    <li>README.pdf</li>\n",
    "    <li>README.md</li>\n",
    "    </ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\20220600_AIML_BirdCounting'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# Set working directory here\n",
    "os.chdir(r'D:\\20220600_AIML_BirdCounting')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import json\n",
    "import yaml\n",
    "import pylab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from random import seed, shuffle, sample\n",
    "from shutil import copyfile, move\n",
    "from subprocess import run, Popen\n",
    "from yaml import safe_load\n",
    "\n",
    "from utils.Slicer import Slicer\n",
    "from utils.utils import ensure_path\n",
    "from utils.data_utils import CocoDataset\n",
    "from utils.utils import str2bool\n",
    "from utils.count import count_folder\n",
    "from utils.train_utils import get_validation_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./params.yaml', 'r') as params_file:\n",
    "        params = yaml.safe_load(params_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split raw dataset \n",
    "\n",
    "This module takes raw images and a point file created in dotdotgoose to split them into appropriate size slices for training.\n",
    "\n",
    "Slice size, as well as raw image size must be specified in the .yaml read in in the 2nd code chunk.\n",
    "\n",
    "The slices are automatically placed into a new directory to be ingested by the 'prepare_training_set.py' module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_slices(slice_dict, input_dict, input_dir, output_dir, datatype):\n",
    "    \"\"\"\n",
    "    This function takes a list of inputs, and for each input does the following:\n",
    "        - Reads the file as an image\n",
    "        - For each slice in slice_set, crops the image and saves the output\n",
    "    \"\"\"    \n",
    "    # Generate and save slices for each file in input dictionary\n",
    "    output_dir = ensure_path(output_dir)\n",
    "    for filename, slice_set in tqdm(input_dict.items(), desc=f\"Saving {datatype} slices\"):\n",
    "        img = cv2.imread(str(input_dir/filename))\n",
    "        for slice_id in slice_set:\n",
    "            l,t,r,b = slice_dict[slice_id]\n",
    "            slice = img[t:b, l:r]\n",
    "            slicename = (os.path.splitext(filename)[0] + f\"_t{t:04d}_b{b:04d}_l{l:04d}_r{r:04d}.jpg\")\n",
    "            slicepath = str(Path(output_dir) / slicename)\n",
    "            cv2.imwrite(slicepath, slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(params, project_dir, points_dict, trainval_dir, holdouts_dir, nolabels_dir):\n",
    "    \"\"\"\n",
    "    This function is the main workhorse in reading the raw project folder\n",
    "    and splitting it up into training, holdout, and nolabels sub-folders.\n",
    "    \"\"\"\n",
    "    # Use the holdouts ratio defined in params.yaml to split the dataset\n",
    "    holdouts_ratio = params['data']['holdouts_ratio']\n",
    "\n",
    "    # Create the folders, if they do not already exist\n",
    "    for path in [trainval_dir, holdouts_dir, nolabels_dir]:\n",
    "        ensure_path(path)\n",
    "\n",
    "    # Separate all files into empty and nonempty files, saving the former into the nolabels folder\n",
    "    nonempty_filenames = []\n",
    "    for filename, _ in tqdm(points_dict.items(), desc=\"Filtering out empty images\"):\n",
    "        points = points_dict[filename]\n",
    "        if len(points) == 0:\n",
    "            copyfile(project_dir/filename, nolabels_dir/filename)\n",
    "        else:\n",
    "            copyfile(project_dir/filename, trainval_dir/filename)\n",
    "            nonempty_filenames.append(filename)\n",
    "\n",
    "    # For all remaining (nonempty) files, split them up into training set and holdout set\n",
    "    seed(1)\n",
    "    shuffle(nonempty_filenames)\n",
    "    nonempty_size = len(nonempty_filenames)\n",
    "    holdouts_size = int(holdouts_ratio*nonempty_size)\n",
    "    holdouts_set = nonempty_filenames[:holdouts_size]\n",
    "\n",
    "    # This is where files are actually moved out of the trainval folder into the holdout folder\n",
    "    for filename in tqdm(holdouts_set, desc=\"Creating holdouts set\"):\n",
    "        move(trainval_dir/filename, holdouts_dir/filename)\n",
    "\n",
    "    return holdouts_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_set(params, points_dict, holdouts_set, \n",
    "                          trainval_input_dir, holdouts_input_dir,\n",
    "                          trainval_output_dir, holdouts_output_dir):\n",
    "    \"\"\"\n",
    "    Once the dataset has been separated into trainval, holdout and nolabels directories,\n",
    "    this function splits each image inside the trainval and holdout directories into pieces (slices).\n",
    "    Saving those slices into respective folders is done in preparation for data labelling.\n",
    "    \"\"\"\n",
    "    # Initialise image and slice dimensions\n",
    "    image_height = params['data']['image_height']\n",
    "    image_width = params['data']['image_width']\n",
    "    slice_height = params['slices']['height']\n",
    "    slice_width = params['slices']['width']\n",
    "\n",
    "    sl = Slicer(h=image_height, w=image_width, dh=slice_height, dw=slice_width)\n",
    "    slice_dict = sl.get_slice_dict()\n",
    "\n",
    "    trainval_sliced_dict = {}\n",
    "    holdouts_sliced_dict = {}\n",
    "    for img_id, coords_dict in tqdm(points_dict.items(), desc=\"Creating slices dictionary\"):\n",
    "        if img_id not in holdouts_set:\n",
    "            slice_list = []\n",
    "            for _, pts in coords_dict.items():\n",
    "                for pt in pts:\n",
    "                    slice_id = sl.get_slice_id_from_point(pt)\n",
    "                    slice_list.append(slice_id)\n",
    "            slice_set = set(slice_list)\n",
    "            if slice_set:\n",
    "                trainval_sliced_dict[img_id] = slice_set\n",
    "        else:\n",
    "            slice_list = []\n",
    "            slice_set = set(slice_dict.keys())\n",
    "            holdouts_sliced_dict[img_id] = slice_set\n",
    "\n",
    "    _save_slices(slice_dict, trainval_sliced_dict, trainval_input_dir, trainval_output_dir, datatype='trainval')\n",
    "    _save_slices(slice_dict, holdouts_sliced_dict, holdouts_input_dir, holdouts_output_dir, datatype='holdouts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering out empty images: 100%|██████████| 6/6 [00:00<00:00,  7.98it/s]\n",
      "Creating holdouts set: 0it [00:00, ?it/s]\n",
      "Creating slices dictionary: 100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "Saving trainval slices: 100%|██████████| 6/6 [00:03<00:00,  1.92it/s]\n",
      "Saving holdouts slices: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialise all required variables\n",
    "raw_dir = Path(params['data']['raw_dir'])\n",
    "project_name = params['data']['project_name']\n",
    "project_folder = raw_dir / project_name\n",
    "trainval_dir = Path(params['data']['trainval_dir'])/project_name\n",
    "holdouts_dir = Path(params['data']['holdouts_dir'])/project_name\n",
    "nolabels_dir = Path(params['data']['nolabels_dir'])/project_name\n",
    "trainval_output_dir = Path(params['slices']['trainval_dir'])/project_name/'sliced_images'\n",
    "holdouts_output_dir = Path(params['slices']['holdouts_dir'])/project_name/'sliced_images'\n",
    "\n",
    "# The .pnt file is necessary to prepare a training set.\n",
    "# Otherwise the number of slices quickly becomes intractable.\n",
    "points_file = [file for file in os.listdir(project_folder) if file.endswith('.pnt')][0]\n",
    "points_path = project_folder / points_file\n",
    "try:\n",
    "    with open(points_path) as file:\n",
    "        data = file.read()\n",
    "        data = json.loads(data)\n",
    "        points_dict = data.get('points')\n",
    "except FileNotFoundError as err:\n",
    "    print(f\"{err}\")\n",
    "    raise\n",
    "except NameError as err:\n",
    "    print(f\"{err}\")\n",
    "    print(f\"Ensure that the .pnt file is properly formatted\")\n",
    "    raise\n",
    "\n",
    "# First split the raw images into categories: trainval, holdouts, nolabels\n",
    "holdouts_set = split_dataset(params, raw_dir / project_name,\n",
    "                                points_dict, trainval_dir,\n",
    "                                holdouts_dir, nolabels_dir)\n",
    "\n",
    "# Then, for the trainval and holdouts datasets, split them up and save the slices\n",
    "# that have been identified by the .pnt file as containing objects. This is done to\n",
    "# filter out empty slices and reduce the labelling required for building a detector.\n",
    "generate_training_set(params, points_dict, holdouts_set, \n",
    "                        trainval_dir, holdouts_dir,\n",
    "                        trainval_output_dir, holdouts_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start label studio\n",
    "\n",
    "Please note that this code will only run using a bash or zsh shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_labeling_job(trainval_dir):\n",
    "    cmd = [\"chmod\", \"+x\", \"./scripts/get_urls.sh\"]\n",
    "    run(cmd)\n",
    "\n",
    "    cmd = [\"./scripts/get_urls.sh\", f\"{trainval_dir}\", \"*.jpg\"]\n",
    "    Popen(cmd)\n",
    "\n",
    "    cmd = [\"label-studio\", \"start\"]\n",
    "    Popen(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training set\n",
    "\n",
    "This script is used to scan through labels produced from label studio \n",
    "and prepare a dataset in the COCO format required for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_slices(filepath, slice_ids, slice_dict, output_dir):\n",
    "    \"\"\"\n",
    "    This function loads an image, slices it up, and saves the pieces (slices) as separate images.\n",
    "    It returns the paths of each slice as a list.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(filepath)\n",
    "    filename = os.path.basename(filepath)\n",
    "    slicepaths = []\n",
    "    for slice_id in slice_ids:\n",
    "        l,t,r,b = slice_dict[slice_id]\n",
    "        slice = img[t:b, l:r]\n",
    "        slicename = (os.path.splitext(filename)[0] + f\"_t{t:04d}_b{b:04d}_l{l:04d}_r{r:04d}.jpg\")\n",
    "        slicepath = str(Path(output_dir) / slicename)\n",
    "        cv2.imwrite(slicepath, slice)\n",
    "        slicepaths.append(slicepath)\n",
    "    return slicepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cocofy_annotations(data_dir, labels):\n",
    "    \"\"\"\n",
    "    This function is used to reformat the annotations from label-studio into that required for training.\n",
    "\n",
    "    Inputs:\n",
    "        - data_dir: Data directory where original slice is located\n",
    "        - labels: Label-studio annotations\n",
    "\n",
    "    Outpus:\n",
    "        - annotations: Dictionary containing COCO formatted annotations\n",
    "    \"\"\"\n",
    "    annotations = {}\n",
    "    for label in labels:\n",
    "        lbl_annotations = label['annotations'][0]\n",
    "        if not lbl_annotations['was_cancelled']:\n",
    "            # This if statement is meant to deal with issues that may arise with label studio if saving over a project\n",
    "            if len(lbl_annotations['result'])==0 and lbl_annotations.get('prediction'):\n",
    "                old_result = lbl_annotations['prediction']['result']\n",
    "                lbl_annotations['result'] = old_result if len(old_result) > 3 else lbl_annotations['result']\n",
    "            \n",
    "            # Convert label studio annotations to COCO format\n",
    "            old_bbox_data = lbl_annotations['result']\n",
    "            coco_bbox_data = []\n",
    "            for bbox in old_bbox_data:\n",
    "                old_bbox = bbox['value']\n",
    "                coco_bbox = [old_bbox['x']/100, old_bbox['y']/100, old_bbox['width']/100, old_bbox['height']/100]\n",
    "                coco_label = old_bbox['rectanglelabels'][0]\n",
    "                coco_bbox_datum = {'bbox': coco_bbox, 'label': coco_label}\n",
    "                coco_bbox_data.append(coco_bbox_datum)\n",
    "\n",
    "            filename = os.path.basename(label['data']['image'])\n",
    "            annotations[str(Path(data_dir) / filename)] = coco_bbox_data\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_set(params):\n",
    "    \"\"\"\n",
    "    This function scans through the labels produced from label studio \n",
    "    and prepares a dataset in a format required for training\n",
    "\n",
    "    Inputs: params (see params.yaml)\n",
    "\n",
    "    Outputs: a dataset in COCO format located in training->train_coco (see params.yaml)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise required variables (see params.yaml)\n",
    "    train_data_dir = ensure_path(params['training']['train_data_dir'])\n",
    "    train_coco = params['training']['train_coco']\n",
    "    project_name = params['data']['project_name']\n",
    "    nolabels_dir = Path(params['data']['nolabels_dir'])/project_name\n",
    "    slices_trainval_dir = Path(params['slices']['trainval_dir'])/project_name/'sliced_images'\n",
    "    slices_nolabels_dir = Path(params['slices']['nolabels_dir'])/project_name/'sliced_images'\n",
    "\n",
    "    image_height = params['data']['image_height']\n",
    "    image_width = params['data']['image_width']\n",
    "    slice_height = params['slices']['height']\n",
    "    slice_width = params['slices']['width']\n",
    "\n",
    "    # This labels variable is the filepath containing the output of the label-studio labelling job.\n",
    "    labels = params['slices']['labels']\n",
    "\n",
    "    # This function transforms, or re-formats, the labels into the required format for training.\n",
    "    with open(labels, 'r') as fr:\n",
    "        annotations = cocofy_annotations(slices_trainval_dir, json.loads(fr.read()))\n",
    "\n",
    "    # This function adds the new annotations to a dataset ready for training\n",
    "    coco_dataset = CocoDataset(train_data_dir)\n",
    "    coco_dataset.add_existing_annotations(annotations)\n",
    "\n",
    "    # This loop scans through the empty images (those containing no labels), slices them up, then\n",
    "    # subsamples a few slices to include in the training set to reduce False Positive Rate.\n",
    "    sl = Slicer(h=image_height, w=image_width, dh=slice_height, dw=slice_width)\n",
    "    slice_dict = sl.get_slice_dict()\n",
    "    output_dir = ensure_path(slices_nolabels_dir)\n",
    "    i = 0\n",
    "    for filename in tqdm(os.listdir(nolabels_dir), desc=\"Adding negative samples\"):\n",
    "        i+=1\n",
    "        seed(i)\n",
    "        filepath = str(Path(nolabels_dir) / filename)\n",
    "        slice_ids = sample(list(slice_dict), 3)\n",
    "        slicepaths = save_slices(filepath, slice_ids, slice_dict, output_dir)\n",
    "        coco_dataset.add_negative_samples(slicepaths)\n",
    "    \n",
    "    # The dataset is exported into a format ready for training\n",
    "    coco_dataset.export_dataset(train_coco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding existing annotations: 100%|██████████| 2/2 [00:00<00:00, 64.51it/s]\n",
      "Adding negative samples: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "prepare_training_set(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from utils.train_utils import collate_fn, get_train_transform, BirdDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    \"\"\"\n",
    "    This is the \"training loop\", which calculates the loss for each epoch\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    for images, labels, filenames in tqdm(data_loader, desc=\"Training\"):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        labels = [{k: v.to(device) for k, v in l.items()} for l in labels]\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, labels)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += losses.item()\n",
    "        del images, labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    train_loss = running_loss / float(len(data_loader))\n",
    "    scheduler.step(train_loss)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop_fn(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    This is the \"validation loop\", which calculates the loss for each epoch\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    for images, labels, filenames in tqdm(data_loader, desc=\"Validation\"):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        labels = [{k: v.to(device) for k, v in l.items()} for l in labels]\n",
    "        loss_dict = model(images, labels)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        running_loss += losses.item()\n",
    "        del images , labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    valid_loss = running_loss / float(len(data_loader))\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mdba(params):\n",
    "    \"\"\"\n",
    "    This is the main function that actually performs the training.\n",
    "\n",
    "    Inputs: params from params.yaml file\n",
    "\n",
    "    Outputs: a best model is saved in the best_model_path\n",
    "    \"\"\"\n",
    "\n",
    "    # Define training parameters, typically no need to change these\n",
    "    lr = params['training']['learning_rate']\n",
    "    wd = params['training']['weight_decay']\n",
    "    momentum = params['training']['momentum']\n",
    "    early_stopping_patience = params['training']['early_stopping_patience']\n",
    "    num_epochs = params['training']['num_epochs']\n",
    "\n",
    "    best_model_path = Path(params['models']['best_model']) # Where output model will be saved\n",
    "    ensure_path(params['models']['weights_dir']) # Ensure the output model folder exists\n",
    "\n",
    "    # Define the dataset using training transforms (see ./utils/train_utils.py)\n",
    "    # NOTE: it may be useful to split train and validation transforms before this step\n",
    "    my_dataset = BirdDataset(\n",
    "        root=params['training']['train_data_dir'],\n",
    "        annotation=params['training']['train_coco'],\n",
    "        transforms=get_train_transform())\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    val_ratio = params['training']['val_ratio']\n",
    "    total_size = len(my_dataset)\n",
    "    val_size = int(val_ratio*total_size)\n",
    "    train_size = total_size-val_size\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(\n",
    "        my_dataset, (train_size, val_size))\n",
    "\n",
    "    # Definte data loaders for training and validation sets\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params['training']['train_batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=params['training']['num_workers_dl'],\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=params['training']['valid_batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=params['training']['num_workers_dl'],\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "    # Load the model into memory\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    num_classes = 2\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer and scheduler\n",
    "    model_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(model_params, lr=lr, momentum=momentum, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "\n",
    "    # Run main training loop, save a model when an epoch outperforms previous best\n",
    "    all_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch --> {epoch+1} / {num_epochs}\")\n",
    "        print(f\"-------------------------------\")\n",
    "        train_loss = train_loop_fn(train_loader, model, optimizer, device, scheduler)\n",
    "        print('training Loss: {:.4f}'.format(train_loss))\n",
    "        valid_loss = eval_loop_fn(valid_loader, model, device)\n",
    "        print('validation Loss: {:.4f}'.format(valid_loss))\n",
    "        all_losses.append(valid_loss)\n",
    "        \n",
    "        ## Uncomment this line to save model after every epoch\n",
    "        # torch.save(model, f\"{weights_dir}/epoch-{epoch}_lr-{lr}.pth\")\n",
    "\n",
    "        if epoch == 0: # Save model as \"best model\" after first epoch\n",
    "            torch.save(model, best_model_path)\n",
    "            best_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "        elif valid_loss < best_loss: # Save model as \"best model\" if it results in lower loss\n",
    "            torch.save(model, best_model_path)\n",
    "            best_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "        elif epoch - best_epoch == early_stopping_patience: # Stop if no progress has been made\n",
    "            print(f\"Early stopping condition reached. Stopping training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Epoch --> 1 / 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]c:\\Users\\a1629970\\.conda\\envs\\ML\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Training: 100%|██████████| 1/1 [00:04<00:00,  4.81s/it]\n",
      "c:\\Users\\a1629970\\.conda\\envs\\ML\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.9810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:11<00:00, 11.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.9202\n",
      "Epoch --> 2 / 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:02<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.7060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:07<00:00,  7.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.7058\n",
      "Epoch --> 3 / 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:03<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.4751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:04<00:00,  4.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.6058\n",
      "Epoch --> 4 / 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:09<00:00,  9.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.4144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:04<00:00,  4.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.5791\n",
      "Epoch --> 5 / 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:09<00:00,  9.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:04<00:00,  4.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.5637\n",
      "Epoch --> 6 / 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:09<00:00,  9.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:04<00:00,  4.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.6021\n",
      "Epoch --> 7 / 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:09<00:00,  9.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.6110\n",
      "Epoch --> 8 / 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:09<00:00,  9.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1/1 [00:11<00:00, 11.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.5962\n",
      "Early stopping condition reached. Stopping training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_mdba(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count birds with trained model\n",
    "\n",
    "After a model has been trained to count species, this script performs the counting.\n",
    "It does so by calling a function within ./utils/count.py that slices images then feeds\n",
    "these slices into the model, one by one, before tallying up the counts and outputting them.\n",
    "\n",
    "Results are saved in an output folder defined below (./model/results by default).\n",
    "\n",
    "Necessary inputs (see params.yaml file for defaults):\n",
    "<ul>\n",
    "    <li>data-root: Root directory (usually ./data/raw) where projects (folders) containing raw images is stored </li>\n",
    "    <li>project-name: Name of the folder where raw images are stored, which will act as inputs for prediction.</li>\n",
    "    <li>image-height: This is the height (in pixels) of raw images (must be consistent across all images).</li>\n",
    "    <li>image-width: This is the width (in pixels) of raw images (must be consistent across all images).</li>\n",
    "    <li>slice-height: This is the height (in pixels) of individual slices. These slices are used for inference.</li>\n",
    "    <li>slice-width: This is the width (in pixels) of individual slices. These slices are used for inference.</li>\n",
    "    <li>overlap: Vertical overlap ratio between adjacent images. Depends on the mission.</li>\n",
    "    <li>sidelap: Horizontal overlap ratio between adjacent images. Depends on the mission.</li>\n",
    "    <li>categories: Categories to detect. See example function call above to deal with multiple categories.</li>\n",
    "        <ul>\n",
    "        <li>-- NOTE: If using customised categories (not recommended!):</li>\n",
    "            <ul>\n",
    "            <li>number of categories must match the number that the model was trained against</li>\n",
    "            <li>categories must be a list of dictionaries with this format:</li>\n",
    "            </ul>\n",
    "                <ul>\n",
    "                <li>- e.g. [\n",
    "                    {'id': 0, 'name': 'bird', 'supercategory': 'animal'},\n",
    "                    {'id': 1, 'name': 'dog', 'supercategory': 'animal'},\n",
    "                ]</li>\n",
    "                </ul>\n",
    "        </ul>\n",
    "</ul>\n",
    "\n",
    "Optional inputs (see params.yaml file for defaults)\n",
    "<ul>\n",
    "    <li>output-folder: This is the folder where outputs will be stored.</li>\n",
    "    <li>conf-thresh: Binary cut-off threshold for detection. Can be used to calibrate for under/overcounting.</li>\n",
    "    <li>stride: Lower values increase computational time for improved performance. Must be > 0 and <=1.</li>\n",
    "    <li>model-path: Model file path. Either update the params.yaml file or the default one in this function.</li>\n",
    "    <li>save-slices: Setting this argument to True saves all the individual slices (inputs). Useful for analysis.</li>\n",
    "</ul>\n",
    "\n",
    "Outputs:\n",
    "<ul>\n",
    "    <li>Look for outputs in the value defined in params.yaml (evaluation->results_dir) or define a new location here.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(params):\n",
    "    \"\"\"\n",
    "    Defining input arguments\n",
    "    \"\"\"\n",
    "\n",
    "    # See params.yaml for these values located in their respective tree. \n",
    "    # e.g. model_path is the value of (models -> best_model) inside params.yaml \n",
    "    raw_dir = Path(params['data']['raw_dir'])\n",
    "    project_name = params['data']['project_name']\n",
    "    model_path = Path(params['models']['best_model'])\n",
    "    results_dir = Path(params['evaluation']['results_dir'])\n",
    "    conf_thresh = params['evaluation']['conf_thresh']\n",
    "    overlap = params['evaluation']['overlap']\n",
    "    sidelap = params['evaluation']['sidelap']\n",
    "    stride = params['evaluation']['stride']\n",
    "\n",
    "    # Initialise image and slice dimensions\n",
    "    image_height = params['data']['image_height']\n",
    "    image_width = params['data']['image_width']\n",
    "    slice_height = params['slices']['height']\n",
    "    slice_width = params['slices']['width']\n",
    "\n",
    "    # This file is generated when preparing the dataset before training.\n",
    "    # See prepare_training_set() function in prepare_training_set.py for details.\n",
    "    with open(params['training']['train_coco'], 'r') as labels_file:\n",
    "        categories = json.load(labels_file).get('categories')\n",
    "\n",
    "    # Arguments for use in command line interaction with counting script\n",
    "    parser = argparse.ArgumentParser(description='Compare predicted vs true counts')\n",
    "    parser.add_argument('--data-root', type=str,\n",
    "                        default=raw_dir, help=\"Root of the data directory (default: ./data/raw/)\")\n",
    "    parser.add_argument('--project-name', type=str,\n",
    "                        default=project_name, help='Project (image) directory')\n",
    "    parser.add_argument('--image-height', type=int,\n",
    "                        default=image_height, help='Height of raw images.')\n",
    "    parser.add_argument('--image-width', type=int,\n",
    "                        default=image_width, help='Width of raw images.')\n",
    "    parser.add_argument('--slice-height', type=int,\n",
    "                        default=slice_height, help='Height of slices.')\n",
    "    parser.add_argument('--slice-width', type=int,\n",
    "                        default=slice_width, help='Width of slices.')\n",
    "    parser.add_argument('--output-folder', type=str,\n",
    "                        default=results_dir, help='Folder to output results')\n",
    "    parser.add_argument('--conf-thresh', type=float,\n",
    "                        default=conf_thresh, help='Binary (cut-off) confidence threshold (default: 0.95)')\n",
    "    parser.add_argument('--overlap', type=float,\n",
    "                        default=overlap, help='Overlap ratio from 0 to 1 (default: 0.1)')\n",
    "    parser.add_argument('--sidelap', type=float,\n",
    "                        default=sidelap, help='Sidelap ratio from 0 to 1 (default: 0.1)')\n",
    "    parser.add_argument('--stride', type=float,\n",
    "                        default=stride, help='Stride (or overlap) ratio between slices (top and bottom)')\n",
    "    parser.add_argument('--model-path', type=str,\n",
    "                        default=model_path, help='Path to model (default: ./models/detection/weights/best.pth)')\n",
    "    parser.add_argument('--categories', type=list,\n",
    "                        default=categories, help='Categories from labels.json')\n",
    "    parser.add_argument('--save-slices', type=str2bool,\n",
    "                        default=False, help='Save slices (default: False). Note: Only set to true when debugging')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args(params)\n",
    "\n",
    "# Execute counting script, located in utils/count.py\n",
    "count_folder(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate trained model\n",
    "\n",
    "If ground truth counts are provided, this module performs linear regression using predicted and manual counts as input\n",
    "\n",
    "There are two outputs:\n",
    "<ul>\n",
    "    <li> A csv containing the predicted and manual counts </li>\n",
    "    <li> A .png figure of the linear regression labelled with the equation and r-squared</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mdba(params):\n",
    "    \"\"\"\n",
    "    This function takes the project name in params.yaml as a parameter, then runs\n",
    "    an inference over all the slices (images) in the holdout set for that project.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise inputs\n",
    "    project_name = params['data']['project_name']\n",
    "    slice_holdout_dir = Path(params['slices']['holdouts_dir'])/project_name/'sliced_images'\n",
    "    results_dir = ensure_path(Path(params['evaluation']['results_dir']))\n",
    "    conf_thresh = params['evaluation']['conf_thresh']\n",
    "\n",
    "    # Load model\n",
    "    model = torch.load(params['models']['best_model'])\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    transform = get_validation_transform()\n",
    "\n",
    "    # For each file in the holdout set, make a prediction and \n",
    "    # store the outputs in a dictionary called predictions\n",
    "    predictions = {}\n",
    "    filenames = sorted(os.listdir(slice_holdout_dir))\n",
    "    img_ids = []\n",
    "    first=True\n",
    "    for filename in tqdm(filenames, desc=\"Counting birds in holdout set\"):\n",
    "        img_id = '_'.join(filename.split('_')[:3]) + '.JPG'\n",
    "        if predictions.get(img_id) is None:\n",
    "            predictions[img_id] = 0\n",
    "            img_ids.append(img_id)\n",
    "            if not first:\n",
    "                print(f\"Count for {prev_img_id} == {predictions[prev_img_id]}\")\n",
    "            first=False\n",
    "        filepath = Path(slice_holdout_dir)/filename\n",
    "        image = Image.open(filepath)\n",
    "        image = transform(image).unsqueeze(0).cuda()\n",
    "\n",
    "        # This line is where the actual \"inference\" occurs.\n",
    "        # Write functionality after this line for customised analysis.\n",
    "        # This variable is a dictionary with three keys: \"boxes\", \"labels\" and \"scores\".\n",
    "        out = model(image)[0]\n",
    "\n",
    "        scores = out['scores'].cpu().detach().numpy()\n",
    "        predictions[img_id] += len(scores[scores>conf_thresh])\n",
    "        prev_img_id = img_id\n",
    "    \n",
    "    print(f\"Count for {prev_img_id} == {predictions[prev_img_id]}\")\n",
    "    ground_truth = {}\n",
    "    ground_truth_data = params['data']['ground_truth_counts']\n",
    "    with open(ground_truth_data, \"r\") as csv_file:\n",
    "        gt_data = csv.reader(csv_file)\n",
    "        for row in gt_data:\n",
    "            if row[0] in img_ids:\n",
    "                ground_truth[row[0]] = int(row[1])\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for key, value in predictions.items():\n",
    "        xs.append(ground_truth[key])\n",
    "        ys.append(value)\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    xs_fit = xs[:,np.newaxis]\n",
    "    a, res, _, _ = np.linalg.lstsq(xs_fit, ys, rcond=None)\n",
    "    r2 = 1 - res / (ys.size * ys.var())\n",
    "\n",
    "    # Output CSV and PNG\n",
    "    output_basename = \"true_vs_predicted_counts\"\n",
    "    dataset = pd.DataFrame({'MANUAL': xs, 'AUTOMATIC': ys}, columns=['MANUAL', 'AUTOMATIC'])\n",
    "    dataset.to_csv(f\"{results_dir}/{output_basename}.csv\")\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "    plt.rcParams.update({'font.size': 25})\n",
    "    pylab.plot(xs,ys,'o')\n",
    "    pylab.plot(xs,a*xs,\"r--\")\n",
    "    plt.xlim(0.9,2200)\n",
    "    plt.ylim(0.9,2200) \n",
    "    plt.ylabel('Predicted Count')\n",
    "    plt.xlabel('True Count')\n",
    "    plt.title('Comparing Ground Truth and Predicted Counts (Holdout Set)')\n",
    "    plt.text(650, 750, f\"y={a[0]:.3f}x,\" + \" $\\mathregular{R^{2}}$\" + f\"= {r2[0]:0.3f}\", rotation = 27.5)\n",
    "    plt.savefig(f\"{results_dir}/{output_basename}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mdba(params)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8975dbe95a9696821a508f700b23ea164b3f8fcfb057d4e51a45b47dea22a8ca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
