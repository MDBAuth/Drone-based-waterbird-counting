{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDBA Machine Learning Notebook\n",
    "\n",
    "Annotating and training an object detector for automated counting of species\n",
    "\n",
    "IMPORTANT NOTE: Parameters in params.yaml are used throughout the notebook.\n",
    "\n",
    "The paths used throughout the notebook are relative and therefore the working directory must be the root of the machine learning directory. E.g.\n",
    "\n",
    "<ul>\n",
    "<li>Root</li>\n",
    "    <ul>\n",
    "    <li>Data</li>\n",
    "    <li>Models</li>\n",
    "    <li>Scripts</li>\n",
    "    <li>Utils</li>\n",
    "    <li>requirements.txt</li>\n",
    "    <li>params.yaml</li>\n",
    "    <li>README.pdf</li>\n",
    "    <li>README.md</li>\n",
    "    </ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set working directory here\n",
    "os.chdir(r'/home/azureuser/cloudfiles//code/Users/Ahsanul.Habib/WaterbirdCount/Drone-based-waterbird-counting')\n",
    "os.getcwd()\n",
    "!export PYTHONPATH=$rootfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import cv2\n",
    "import codecs,json\n",
    "import yaml\n",
    "import pylab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from json import JSONEncoder\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from random import seed, shuffle, sample\n",
    "from shutil import copyfile, move\n",
    "from subprocess import run, Popen\n",
    "from yaml import safe_load\n",
    "\n",
    "from utils.Slicer import Slicer\n",
    "from utils.utils import ensure_path\n",
    "from utils.data_utils import CocoDataset\n",
    "from utils.utils import str2bool\n",
    "from utils.count import count_folder\n",
    "from utils.train_utils import get_transform\n",
    "# !pip install torchmetrics[detection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./params.yaml', 'r') as params_file:\n",
    "        params = yaml.safe_load(params_file)\n",
    "# print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split raw dataset \n",
    "\n",
    "This module takes raw images and a point file created in dotdotgoose to split them into appropriate size slices for training.\n",
    "\n",
    "Slice size, as well as raw image size must be specified in the .yaml read in in the 2nd code chunk.\n",
    "\n",
    "The slices are automatically placed into a new directory to be ingested by the 'prepare_training_set.py' module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_slices(params, slice_dict, input_dict, input_dir, output_dir, datatype):\n",
    "    \"\"\"\n",
    "    This function takes a list of inputs, and for each input does the following:\n",
    "        - Reads the file as an image\n",
    "        - For each slice in slice_set, crops the image and saves the output\n",
    "    \"\"\"    \n",
    "    # Generate and save slices for each file in input dictionary\n",
    "    \n",
    "    output_dir = ensure_path(output_dir)\n",
    "    for filename, slice_set in tqdm(input_dict.items(), desc=f\"Saving {datatype} slices\"):\n",
    "        img = cv2.imread(str(input_dir/filename))\n",
    "        for slice_id in slice_set:\n",
    "            l,t,r,b = slice_dict[slice_id]\n",
    "            slice = img[t:b, l:r]\n",
    "            slicename = (os.path.splitext(filename)[0] + f\"_t{t:04d}_b{b:04d}_l{l:04d}_r{r:04d}.jpg\")\n",
    "            slicepath = str(Path(output_dir) / slicename)\n",
    "            cv2.imwrite(slicepath, slice)\n",
    "            if datatype == 'trainval':\n",
    "                cv2.imwrite(str(Path(params['training']['train_data_dir'])/params['data']['project_name']/'sliced_images'/slicename), slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(params, project_dir, points_dict, trainval_dir, holdouts_dir, nolabels_dir, *args):\n",
    "    \"\"\"\n",
    "    This function is the main workhorse in reading the raw project folder\n",
    "    and splitting it up into training, holdout, and nolabels sub-folders.\n",
    "    \"\"\"\n",
    "    # Use the holdouts ratio defined in params.yaml to split the dataset\n",
    "    holdouts_ratio = params['data']['holdouts_ratio']\n",
    "\n",
    "    # Create the folders, if they do not already exist\n",
    "    for path in [trainval_dir, holdouts_dir, nolabels_dir]:\n",
    "        ensure_path(path)\n",
    "\n",
    "    # Separate all images without annotation data\n",
    "    noannotation_filenames = args[0]\n",
    "    withannotation_filenames = {}\n",
    "    for filename, _ in tqdm(points_dict.items(), desc=\"Filtering out images with missing annotation data\"):\n",
    "        if filename not in noannotation_filenames:\n",
    "            withannotation_filenames[filename] = points_dict[filename]\n",
    "        \n",
    "    # Separate all files into empty and nonempty files, saving the former into the nolabels folder\n",
    "    nonempty_filenames = []\n",
    "    for filename, _ in tqdm(withannotation_filenames.items(), desc=\"Filtering out empty images\"):\n",
    "        points = withannotation_filenames[filename]\n",
    "        if len(points) == 0:\n",
    "            copyfile(project_dir/filename, nolabels_dir/filename)\n",
    "        else:\n",
    "            copyfile(project_dir/filename, trainval_dir/filename)\n",
    "            nonempty_filenames.append(filename)\n",
    "\n",
    "    # For all remaining (nonempty) files, split them up into training set and holdout set\n",
    "    seed(1)\n",
    "    shuffle(nonempty_filenames)\n",
    "    nonempty_size = len(nonempty_filenames)\n",
    "    holdouts_size = int(holdouts_ratio*nonempty_size)\n",
    "    holdouts_set = nonempty_filenames[:holdouts_size]\n",
    "\n",
    "    # This is where files are actually moved out of the trainval folder into the holdout folder\n",
    "    for filename in tqdm(holdouts_set, desc=\"Creating holdouts set\"):\n",
    "        move(trainval_dir/filename, holdouts_dir/filename)\n",
    "\n",
    "    return holdouts_set, withannotation_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_set(params, points_dict, holdouts_set, \n",
    "                          trainval_input_dir, holdouts_input_dir,\n",
    "                          trainval_output_dir, holdouts_output_dir):\n",
    "    \"\"\"\n",
    "    Once the dataset has been separated into trainval, holdout and nolabels directories,\n",
    "    this function splits each image inside the trainval and holdout directories into pieces (slices).\n",
    "    Saving those slices into respective folders is done in preparation for data labelling.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise image and slice dimensions\n",
    "    image_height = params['data']['image_height']\n",
    "    image_width = params['data']['image_width']\n",
    "    slice_height = params['slices']['height']\n",
    "    slice_width = params['slices']['width']\n",
    "\n",
    "    sl = Slicer(h=image_height, w=image_width, dh=slice_height, dw=slice_width)\n",
    "    slice_dict = sl.get_slice_dict()\n",
    "    \n",
    "    trainval_sliced_dict = {}\n",
    "    holdouts_sliced_dict = {}\n",
    "    for img_id, coords_dict in tqdm(points_dict.items(), desc=\"Creating slices dictionary\"):\n",
    "        if img_id not in holdouts_set:\n",
    "            slice_list = []\n",
    "            for _, pts in coords_dict.items():\n",
    "                for pt in pts:\n",
    "                    slice_id = sl.get_slice_id_from_point(pt)\n",
    "                    slice_list.append(slice_id)\n",
    "            slice_set = set(slice_list)\n",
    "            if slice_set:\n",
    "                trainval_sliced_dict[img_id] = slice_set\n",
    "        else:\n",
    "            slice_list = []\n",
    "            slice_set = set(slice_dict.keys())\n",
    "            holdouts_sliced_dict[img_id] = slice_set\n",
    "\n",
    "    _save_slices(params, slice_dict, trainval_sliced_dict, trainval_input_dir, trainval_output_dir, datatype='trainval')\n",
    "    _save_slices(params, slice_dict, holdouts_sliced_dict, holdouts_input_dir, holdouts_output_dir, datatype='holdouts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering out images with missing annotation data: 100%|██████████| 205/205 [00:00<00:00, 1176241.20it/s]\n",
      "Filtering out empty images: 100%|██████████| 188/188 [02:21<00:00,  1.33it/s]\n",
      "Creating holdouts set: 100%|██████████| 15/15 [00:01<00:00, 13.01it/s]\n",
      "Creating slices dictionary: 100%|██████████| 188/188 [00:00<00:00, 3370.47it/s]\n",
      "Saving trainval slices: 100%|██████████| 85/85 [06:39<00:00,  4.70s/it]\n",
      "Saving holdouts slices: 100%|██████████| 15/15 [02:58<00:00, 11.90s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialise all required variables\n",
    "raw_dir = Path(params['data']['raw_dir'])\n",
    "project_name = params['data']['project_name']\n",
    "project_folder = raw_dir / project_name\n",
    "points_file = [file for file in os.listdir(project_folder) if file.endswith('.pnt')][0]\n",
    "points_path = project_folder / points_file\n",
    "\n",
    "holdouts_dir = Path(params['data']['holdouts_dir'])/(project_name+'_v1')\n",
    "filenames = sorted(os.listdir(holdouts_dir))\n",
    "no_annotation = [filename for filename in filenames if filename.lower().endswith('.jpg')]\n",
    "\n",
    "trainval_dir = Path(params['data']['trainval_dir'])/project_name\n",
    "holdouts_dir = Path(params['data']['holdouts_dir'])/project_name\n",
    "nolabels_dir = Path(params['data']['nolabels_dir'])/project_name\n",
    "trainval_output_dir = Path(params['slices']['trainval_dir'])/project_name/'sliced_images'\n",
    "holdouts_output_dir = Path(params['slices']['holdouts_dir'])/project_name/'sliced_images'\n",
    "\n",
    "# The .pnt file is necessary to prepare a training set.\n",
    "# Otherwise the number of slices quickly becomes intractable.\n",
    "points_file = [file for file in os.listdir(project_folder) if file.endswith('.pnt')][0]\n",
    "points_path = project_folder / points_file\n",
    "try:\n",
    "    with open(points_path) as file:\n",
    "        data = file.read()\n",
    "        data = json.loads(data)\n",
    "        points_dict = data.get('points')\n",
    "except FileNotFoundError as err:\n",
    "    print(f\"{err}\")\n",
    "    raise\n",
    "except NameError as err:\n",
    "    print(f\"{err}\")\n",
    "    print(f\"Ensure that the .pnt file is properly formatted\")\n",
    "    raise\n",
    "\n",
    "# First split the raw images into categories: trainval, holdouts, nolabels\n",
    "holdouts_set, updated_points_dict = split_dataset(params, raw_dir / project_name, points_dict, trainval_dir, holdouts_dir, nolabels_dir, no_annotation)\n",
    "points_dict = updated_points_dict\n",
    "# Then, for the trainval and holdouts datasets, split them up and save the slices that have been identified by the .pnt file as containing objects. \n",
    "# This is done to filter out empty slices and reduce the labelling required for building a detector.\n",
    "generate_training_set(params, points_dict, holdouts_set, trainval_dir, holdouts_dir, trainval_output_dir, holdouts_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start label studio\n",
    "\n",
    "Please note that this code will only run using a bash or zsh shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_labeling_job(trainval_dir):\n",
    "    cmd = [\"chmod\", \"+x\", \"./scripts/get_urls.sh\"]\n",
    "    run(cmd)\n",
    "\n",
    "    cmd = [\"./scripts/get_urls.sh\", f\"{trainval_dir}\", \"*.jpg\"]\n",
    "    Popen(cmd)\n",
    "\n",
    "    cmd = [\"label-studio\", \"start\"]\n",
    "    Popen(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training set\n",
    "\n",
    "This script is used to scan through labels produced from label studio \n",
    "and prepare a dataset in the COCO format required for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_slices(filepath, slice_ids, slice_dict, output_dir):\n",
    "    \"\"\"\n",
    "    This function loads an image, slices it up, and saves the pieces (slices) as separate images.\n",
    "    It returns the paths of each slice as a list.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(filepath)\n",
    "    filename = os.path.basename(filepath)\n",
    "    slicepaths = []\n",
    "    for slice_id in slice_ids:\n",
    "        l,t,r,b = slice_dict[slice_id]\n",
    "        slice = img[t:b, l:r]\n",
    "        slicename = (os.path.splitext(filename)[0] + f\"_t{t:04d}_b{b:04d}_l{l:04d}_r{r:04d}.jpg\")\n",
    "        slicepath = str(Path(output_dir) / slicename)\n",
    "        cv2.imwrite(slicepath, slice)\n",
    "        slicepaths.append(slicepath)\n",
    "    return slicepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cocofy_annotations(data_dir, labels, *args):\n",
    "    \"\"\"\n",
    "    This function is used to reformat the annotations from label-studio into that required for training.\n",
    "\n",
    "    Inputs:\n",
    "        - data_dir: Data directory where original slice is located\n",
    "        - labels: Label-studio annotations\n",
    "\n",
    "    Outpus:\n",
    "        - annotations: Dictionary containing COCO formatted annotations\n",
    "    \"\"\"\n",
    "    \n",
    "    excluded_images = args[0]\n",
    "    # trainval_images = args[1]\n",
    "    annotations = {}\n",
    "    for label in labels:\n",
    "        lbl_annotations = label['annotations'][0]\n",
    "        if not lbl_annotations['was_cancelled']:\n",
    "            # This if statement is meant to deal with issues that may arise with label studio if saving over a project\n",
    "            if len(lbl_annotations['result'])==0 and lbl_annotations.get('prediction'):\n",
    "                old_result = lbl_annotations['prediction']['result']\n",
    "                lbl_annotations['result'] = old_result if len(old_result) > 3 else lbl_annotations['result']\n",
    "            \n",
    "            # Convert label studio annotations to COCO format\n",
    "            old_bbox_data = lbl_annotations['result']\n",
    "            coco_bbox_data = []\n",
    "            for bbox in old_bbox_data:\n",
    "                old_bbox = bbox['value']\n",
    "                coco_bbox = [old_bbox['x']/100, old_bbox['y']/100, old_bbox['width']/100, old_bbox['height']/100]\n",
    "                coco_label = old_bbox['rectanglelabels'][0]\n",
    "                coco_bbox_datum = {'bbox': coco_bbox, 'label': coco_label}\n",
    "                coco_bbox_data.append(coco_bbox_datum)\n",
    "\n",
    "            filename = os.path.basename(label['data']['image'])\n",
    "            if not any([file for file in excluded_images if file.split('.JPG')[0] in filename]):\n",
    "                annotations[str(Path(data_dir) / filename)] = coco_bbox_data\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_set(params):\n",
    "    \"\"\"\n",
    "    This function scans through the labels produced from label studio \n",
    "    and prepares a dataset in a format required for training\n",
    "\n",
    "    Inputs: params (see params.yaml)\n",
    "\n",
    "    Outputs: a dataset in COCO format located in training->train_coco (see params.yaml)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise required variables (see params.yaml)\n",
    "    train_data_dir = ensure_path('./'+str(Path(params['training']['train_data_dir'])/params['data']['project_name']/'sliced_images'))\n",
    "    train_coco = params['training']['train_coco']\n",
    "    project_name = params['data']['project_name']\n",
    "    nolabels_dir = Path(params['data']['nolabels_dir'])/project_name\n",
    "    \n",
    "    holdouts_dir0 = Path(params['data']['holdouts_dir'])/(project_name)\n",
    "    holdouts_dir1 = Path(params['data']['holdouts_dir'])/(project_name+'_v1')\n",
    "    \n",
    "    filenames_ho = sorted(os.listdir(holdouts_dir0))\n",
    "    filenames_na = sorted(os.listdir(holdouts_dir1))\n",
    "    \n",
    "    no_annotation = [filename for filename in filenames_na if filename.lower().endswith('.jpg')]\n",
    "    holdoutimages = [filename for filename in filenames_ho if filename.lower().endswith('.jpg')]\n",
    "    \n",
    "    slices_trainval_dir = Path(params['slices']['trainval_dir'])/project_name/'sliced_images'\n",
    "    slices_nolabels_dir = Path(params['slices']['nolabels_dir'])/project_name/'sliced_images'\n",
    "\n",
    "    image_height = params['data']['image_height']\n",
    "    image_width = params['data']['image_width']\n",
    "    slice_height = params['slices']['height']\n",
    "    slice_width = params['slices']['width']\n",
    "\n",
    "    # This labels variable is the filepath containing the output of the label-studio labelling job.\n",
    "    labels = params['slices']['labels']\n",
    "\n",
    "    # This function transforms, or re-formats, the labels into the required format for training.\n",
    "    with open(labels, 'r') as fr:\n",
    "        annotations = cocofy_annotations(slices_trainval_dir, json.loads(fr.read()), no_annotation+holdoutimages)\n",
    "\n",
    "    # This function adds the new annotations to a dataset ready for training\n",
    "    coco_dataset = CocoDataset(train_data_dir)\n",
    "    coco_dataset.add_existing_annotations(annotations)\n",
    "\n",
    "    # This loop scans through the empty images (those containing no labels), slices them up, then\n",
    "    # subsamples a few slices to include in the training set to reduce False Positive Rate.\n",
    "    sl = Slicer(h=image_height, w=image_width, dh=slice_height, dw=slice_width)\n",
    "    slice_dict = sl.get_slice_dict()\n",
    "    output_dir = ensure_path(slices_nolabels_dir)\n",
    "    i = 0\n",
    "    for filename in tqdm(os.listdir(nolabels_dir), desc=\"Adding negative samples\"):\n",
    "        i+=1\n",
    "        seed(i)\n",
    "        filepath = str(Path(nolabels_dir) / filename)\n",
    "        slice_ids = sample(list(slice_dict), 3)\n",
    "        slicepaths = save_slices(filepath, slice_ids, slice_dict, output_dir)\n",
    "        coco_dataset.add_negative_samples(slicepaths)\n",
    "    \n",
    "    # The dataset is exported into a format ready for training\n",
    "    coco_dataset.export_dataset(train_coco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding existing annotations: 100%|██████████| 1759/1759 [03:03<00:00,  9.59it/s]\n",
      "Adding negative samples: 100%|██████████| 88/88 [02:01<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "prepare_training_set(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from utils.train_utils import collate_fn, get_transform, BirdDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    \"\"\"\n",
    "    This is the \"training loop\", which calculates the loss for each epoch\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    for images, labels, filenames in tqdm(data_loader, desc=\"Training\"):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        labels = [{k: v.to(device) for k, v in l.items()} for l in labels]\n",
    "        # print(labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, labels)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += losses.item()\n",
    "        del images, labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    train_loss = running_loss / float(len(data_loader))\n",
    "    scheduler.step(train_loss)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop_fn(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    This is the \"validation loop\", which calculates the loss for each epoch\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    for images, labels, filenames in tqdm(data_loader, desc=\"Validation\"):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        labels = [{k: v.to(device) for k, v in l.items()} for l in labels]\n",
    "        loss_dict = model(images, labels)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        running_loss += losses.item()\n",
    "        del images , labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    valid_loss = running_loss / float(len(data_loader))\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mdba(params):\n",
    "    \"\"\"\n",
    "    This is the main function that actually performs the training.\n",
    "\n",
    "    Inputs: params from params.yaml file\n",
    "\n",
    "    Outputs: a best model is saved in the best_model_path\n",
    "    \"\"\"\n",
    "\n",
    "    # Define training parameters, typically no need to change these\n",
    "    lr = params['training']['learning_rate']\n",
    "    wd = params['training']['weight_decay']\n",
    "    momentum = params['training']['momentum']\n",
    "    early_stopping_patience = params['training']['early_stopping_patience']\n",
    "    num_epochs = params['training']['num_epochs']\n",
    "\n",
    "    best_model_path = Path(params['models']['best_model']) # Where output model will be saved\n",
    "    ensure_path(params['models']['weights_dir']) # Ensure the output model folder exists\n",
    "\n",
    "    # Define the dataset using training transforms (see ./utils/train_utils.py)\n",
    "    # NOTE: it may be useful to split train and validation transforms before this step\n",
    "    my_dataset = BirdDataset(\n",
    "        root=params['training']['train_data_dir'],\n",
    "        annotation=params['training']['train_coco'],\n",
    "        transforms=get_transform())\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    val_ratio = params['training']['val_ratio']\n",
    "    total_size = len(my_dataset)\n",
    "    val_size = int(val_ratio*total_size)\n",
    "    train_size = total_size-val_size\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(\n",
    "        my_dataset, (train_size, val_size))\n",
    "\n",
    "    # Define data loaders for training and validation sets\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params['training']['train_batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=params['training']['num_workers_dl'],\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=params['training']['valid_batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=params['training']['num_workers_dl'],\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "    # Load the model into memory\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    num_classes = 2\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer and scheduler\n",
    "    model_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(model_params, lr=lr, momentum=momentum, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "\n",
    "    # Run main training loop, save a model when an epoch outperforms previous best\n",
    "    all_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch --> {epoch+1} / {num_epochs}\")\n",
    "        print(f\"-------------------------------\")\n",
    "        train_loss = train_loop_fn(train_loader, model, optimizer, device, scheduler)\n",
    "        print('training Loss: {:.4f}'.format(train_loss))\n",
    "        valid_loss = eval_loop_fn(valid_loader, model, device)\n",
    "        print('validation Loss: {:.4f}'.format(valid_loss))\n",
    "        all_losses.append(valid_loss)\n",
    "        \n",
    "        ## Uncomment this line to save model after every epoch\n",
    "        # torch.save(model, f\"{weights_dir}/epoch-{epoch}_lr-{lr}.pth\")\n",
    "\n",
    "        if epoch == 0: # Save model as \"best model\" after first epoch\n",
    "            torch.save(model, best_model_path)\n",
    "            best_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "        elif valid_loss < best_loss: # Save model as \"best model\" if it results in lower loss\n",
    "            torch.save(model, best_model_path)\n",
    "            best_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "        elif epoch - best_epoch == early_stopping_patience: # Stop if no progress has been made\n",
    "            print(f\"Early stopping condition reached. Stopping training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n",
      "Epoch --> 1 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.5051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3892\n",
      "Epoch --> 2 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:59<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3705\n",
      "Epoch --> 3 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3554\n",
      "Epoch --> 4 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3474\n",
      "Epoch --> 5 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3458\n",
      "Epoch --> 6 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3438\n",
      "Epoch --> 7 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.3055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3472\n",
      "Epoch --> 8 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3438\n",
      "Epoch --> 9 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3486\n",
      "Epoch --> 10 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3514\n",
      "Epoch --> 11 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3490\n",
      "Epoch --> 12 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3518\n",
      "Epoch --> 13 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3514\n",
      "Epoch --> 14 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3594\n",
      "Epoch --> 15 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3627\n",
      "Epoch --> 16 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3616\n",
      "Epoch --> 17 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3613\n",
      "Epoch --> 18 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3671\n",
      "Epoch --> 19 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3600\n",
      "Epoch --> 20 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3658\n",
      "Epoch --> 21 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:58<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3750\n",
      "Epoch --> 22 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:59<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3688\n",
      "Epoch --> 23 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:59<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3757\n",
      "Epoch --> 24 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:59<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3775\n",
      "Epoch --> 25 / 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 203/203 [05:59<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 101/101 [00:45<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.3772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_mdba(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count birds with trained model\n",
    "\n",
    "After a model has been trained to count species, this script performs the counting.\n",
    "It does so by calling a function within ./utils/count.py that slices images then feeds\n",
    "these slices into the model, one by one, before tallying up the counts and outputting them.\n",
    "\n",
    "Results are saved in an output folder defined below (./model/results by default).\n",
    "\n",
    "Necessary inputs (see params.yaml file for defaults):\n",
    "<ul>\n",
    "    <li>data-root: Root directory (usually ./data/raw) where projects (folders) containing raw images is stored </li>\n",
    "    <li>project-name: Name of the folder where raw images are stored, which will act as inputs for prediction.</li>\n",
    "    <li>image-height: This is the height (in pixels) of raw images (must be consistent across all images).</li>\n",
    "    <li>image-width: This is the width (in pixels) of raw images (must be consistent across all images).</li>\n",
    "    <li>slice-height: This is the height (in pixels) of individual slices. These slices are used for inference.</li>\n",
    "    <li>slice-width: This is the width (in pixels) of individual slices. These slices are used for inference.</li>\n",
    "    <li>overlap: Vertical overlap ratio between adjacent images. Depends on the mission.</li>\n",
    "    <li>sidelap: Horizontal overlap ratio between adjacent images. Depends on the mission.</li>\n",
    "    <li>categories: Categories to detect. See example function call above to deal with multiple categories.</li>\n",
    "        <ul>\n",
    "        <li>-- NOTE: If using customised categories (not recommended!):</li>\n",
    "            <ul>\n",
    "            <li>number of categories must match the number that the model was trained against</li>\n",
    "            <li>categories must be a list of dictionaries with this format:</li>\n",
    "            </ul>\n",
    "                <ul>\n",
    "                <li>- e.g. [\n",
    "                    {'id': 0, 'name': 'bird', 'supercategory': 'animal'},\n",
    "                    {'id': 1, 'name': 'dog', 'supercategory': 'animal'},\n",
    "                ]</li>\n",
    "                </ul>\n",
    "        </ul>\n",
    "</ul>\n",
    "\n",
    "Optional inputs (see params.yaml file for defaults)\n",
    "<ul>\n",
    "    <li>output-folder: This is the folder where outputs will be stored.</li>\n",
    "    <li>conf-thresh: Binary cut-off threshold for detection. Can be used to calibrate for under/overcounting.</li>\n",
    "    <li>stride: Lower values increase computational time for improved performance. Must be > 0 and <=1.</li>\n",
    "    <li>model-path: Model file path. Either update the params.yaml file or the default one in this function.</li>\n",
    "    <li>save-slices: Setting this argument to True saves all the individual slices (inputs). Useful for analysis.</li>\n",
    "</ul>\n",
    "\n",
    "Outputs:\n",
    "<ul>\n",
    "    <li>Look for outputs in the value defined in params.yaml (evaluation->results_dir) or define a new location here.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/azureuser/cloudfiles//code/Users/Ahsanul.Habib/WaterbirdCount/Drone-based-waterbird-counting\n"
     ]
    }
   ],
   "source": [
    "# find root location useing this notebook's location and set a project folder to get images from and save results to\n",
    "import os as os\n",
    "ipynb_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "rootfolder =ipynb_path.removesuffix(\"/notebooks\")\n",
    "if (rootfolder.startswith(\"/mnt\")): # if using mounted storage in AzureML we need to point to the local path\n",
    "    import re\n",
    "    m = re.search(\"/clusters/[^/]*(.*)\", rootfolder)\n",
    "    rootfolder =\"/home/azureuser/cloudfiles/\" + m.group(1)\n",
    "    print(rootfolder)\n",
    "projectfolder = 'testproject'\n",
    "\n",
    "os.chdir(rootfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = rootfolder + \"/scripts/bird_count.py\" \n",
    "%run $script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate trained model\n",
    "\n",
    "If ground truth counts are provided, this module performs linear regression using predicted and manual counts as input\n",
    "\n",
    "There are two outputs:\n",
    "<ul>\n",
    "    <li> A csv containing the predicted and manual counts </li>\n",
    "    <li> A .png figure of the linear regression labelled with the equation and r-squared</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mdba(params):\n",
    "    \"\"\"\n",
    "    This function takes the project name in params.yaml as a parameter, then runs\n",
    "    an inference over all the slices (images) in the holdout set for that project.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise inputs\n",
    "    project_name = params['data']['project_name']\n",
    "    slice_holdout_dir = Path(params['slices']['holdouts_dir'])/project_name/'sliced_images'\n",
    "    results_dir = ensure_path(Path(params['evaluation']['results_dir']))\n",
    "    conf_thresh = params['evaluation']['conf_thresh']\n",
    "\n",
    "    # Load model\n",
    "    model = torch.load(params['models']['best_model'])\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    transform = get_transform()\n",
    "\n",
    "    # For each file in the holdout set, make a prediction and \n",
    "    # store the outputs in a dictionary called predictions\n",
    "    predictions = {}\n",
    "    filenames = sorted(os.listdir(slice_holdout_dir))\n",
    "    img_ids = []\n",
    "    first=True\n",
    "    for filename in tqdm(filenames, desc=\"Counting birds in holdout set\"):\n",
    "        img_id = '_'.join(filename.split('_')[:3]) + '.JPG'\n",
    "        if predictions.get(img_id) is None:\n",
    "            predictions[img_id] = 0\n",
    "            img_ids.append(img_id)\n",
    "            if not first:\n",
    "                print(f\"Count for {prev_img_id} == {predictions[prev_img_id]}\")\n",
    "            first=False\n",
    "        filepath = Path(slice_holdout_dir)/filename\n",
    "        image = Image.open(filepath)\n",
    "        image = transform(image).unsqueeze(0).cuda()\n",
    "\n",
    "        # This line is where the actual \"inference\" occurs.\n",
    "        # Write functionality after this line for customised analysis.\n",
    "        # This variable is a dictionary with three keys: \"boxes\", \"labels\" and \"scores\".\n",
    "        out = model(image)[0]\n",
    "\n",
    "        scores = out['scores'].cpu().detach().numpy()\n",
    "        predictions[img_id] += len(scores[scores>conf_thresh])\n",
    "        prev_img_id = img_id\n",
    "    \n",
    "    print(f\"Count for {prev_img_id} == {predictions[prev_img_id]}\")\n",
    "    ground_truth = {}\n",
    "    ground_truth_data = params['data']['ground_truth_counts']\n",
    "    with open(ground_truth_data, \"r\") as csv_file:\n",
    "        gt_data = csv.reader(csv_file)\n",
    "        for row in gt_data:\n",
    "            if row[0] in img_ids:\n",
    "                ground_truth[row[0]] = int(row[1])\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for key, value in predictions.items():\n",
    "        xs.append(ground_truth[key])\n",
    "        ys.append(value)\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    xs_fit = xs[:,np.newaxis]\n",
    "    a, res, _, _ = np.linalg.lstsq(xs_fit, ys, rcond=None)\n",
    "    r2 = 1 - res / (ys.size * ys.var())\n",
    "\n",
    "    # Output CSV and PNG\n",
    "    output_basename = \"true_vs_predicted_counts\"\n",
    "    dataset = pd.DataFrame({'MANUAL': xs, 'AUTOMATIC': ys}, columns=['MANUAL', 'AUTOMATIC'])\n",
    "    dataset.to_csv(f\"{results_dir}/{output_basename}.csv\")\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "    plt.rcParams.update({'font.size': 25})\n",
    "    pylab.plot(xs,ys,'o')\n",
    "    pylab.plot(xs,a*xs,\"r--\")\n",
    "    plt.xlim(0.9,2200)\n",
    "    plt.ylim(0.9,2200) \n",
    "    plt.ylabel('Predicted Count')\n",
    "    plt.xlabel('True Count')\n",
    "    plt.title('Comparing Ground Truth and Predicted Counts (Holdout Set)')\n",
    "    plt.text(650, 750, f\"y={a[0]:.3f}x,\" + \" $\\mathregular{R^{2}}$\" + f\"= {r2[0]:0.3f}\", rotation = 27.5)\n",
    "    plt.savefig(f\"{results_dir}/{output_basename}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'f}^\\xbd\\x9c_\\xe6\\xce\\x88_\\x10\\x139+$\\x96\\xe3k S\\x8f\\x99>6\\x8d\\x00h\\xec\\xea\\x89\\xcc\\xd29\\xcd\\xc3\\xf1\\x9b\\xa0\\xc8\\xe7\\xd6)O\\x91\\x85op\\xf4I\\x01\\xb0\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05', b'\\x03\\x08']\n",
      "Bad pipe message: %s [b'\\x08\\x08\\t\\x08\\n\\x08']\n",
      "Bad pipe message: %s [b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'\\xc8B\\xc4\\x19\\xe4\\tY\\xf0\\x87\\xc3,\\xbfw\\xd8\\\\\\xeb\\xb5K \\x13\\xc7\\x07\\xed\\x12\\xeb\\x1ai\\xaf\\xe0\\xf9LC\\x07-U\\xe9']\n",
      "Bad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xf9\\x7f6\\xd6\\xc4(\\x07[\\x89\\xf0h\\xeaA\\x1b\\xbd}I\\xc1\\xc4\\xa1`\\x95']\n",
      "Bad pipe message: %s [b\"\\x91V\\xd2\\xcb-\\xf4J\\xc6v\\x16d\\x01\\xb7`;\\x8a\\xf6i\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\"]\n",
      "Bad pipe message: %s [b'\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00']\n",
      "Bad pipe message: %s [b'g\\xact>\\xd5H\\xfcr\\xb2\\x82\\xea\\xf7b\\x96\\xe6X\\x04;\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b']\n",
      "Bad pipe message: %s [b'CKAQWt\\x1c\\xca\\xd4\\xe9\\xc15C\\x997/\\x07\\xa6']\n",
      "Bad pipe message: %s [b\"&\\xc2\\xc8\\xe0\\xb8\\xaa\\xff4{\\x04.S\\x01\\x0f\\x8dV>\\x9a\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\r\\x00 \\x00\\x1e\\x06\\x01\\x06\\x02\\x06\\x03\\x05\\x01\\x05\\x02\\x05\\x03\\x04\\x01\\x04\\x02\\x04\\x03\\x03\\x01\\x03\\x02\"]\n",
      "Bad pipe message: %s [b'\\x02\\x01', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x0f\\x00']\n",
      "Bad pipe message: %s [b\"vY7\\x86\\xf4\\n\\x90tER\\xb8\\x1e\\x07\\x8f\\xf0\\x18q(\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\"]\n",
      "Bad pipe message: %s [b'\\x15\\xc0\\x0b\\xc0\\x01']\n"
     ]
    }
   ],
   "source": [
    "# eval_mdba(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import yaml\n",
    "import torch\n",
    "import pylab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "from utils.Slicer import Slicer\n",
    "from utils.utils import ensure_path\n",
    "from utils.train_utils import get_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_holdout_slices_with_inferences(parameters, slicename, boxes, Irec):\n",
    "    if len(boxes) > 0:\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i].detach().cpu().numpy()\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            Irec = cv2.rectangle(Irec, (startX, startY), (endX, endY), (0,0,255), 2)\n",
    "        output_folder = ensure_path(Path(parameters['inference']['results_dir'])/(parameters['data']['project_name'] + '_detections'))\n",
    "        output_path = output_folder/f'{slicename}.jpg'\n",
    "        cv2.imwrite(str(output_path), Irec)\n",
    "        # plt.imshow(Irec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_inference_results(out_folder, predictions):\n",
    "    pred_filepath = out_folder/'inference_results.json'\n",
    "    preds = []\n",
    "    for l in range(len(predictions)):\n",
    "        preds.append(dict(filepath=predictions[l]['filepath'],\n",
    "                          boxes=predictions[l]['boxes'].numpy().tolist(),\n",
    "                          scores=predictions[l]['scores'].numpy().tolist(),\n",
    "                          labels=predictions[l]['labels'].numpy().tolist()\n",
    "                         )\n",
    "                    )\n",
    "    json.dump(preds, codecs.open(pred_filepath, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groundTruth_annotations(data_dir, labels, *args):\n",
    "    \"\"\"\n",
    "    This function is used to reformat the annotations from label-studio into that required for training.\n",
    "\n",
    "    Inputs:\n",
    "        - data_dir: Data directory where original slice is located\n",
    "        - labels: Label-studio annotations\n",
    "        - args[0]: A list of excluded images\n",
    "\n",
    "    Outpus:\n",
    "        - annotations: Dictionary containing ground truth annotation\n",
    "    \"\"\"\n",
    "    \n",
    "    excluded_images = args[0]\n",
    "    annotations = []\n",
    "    for label in labels:\n",
    "        lbl_annotations = label['annotations'][0]\n",
    "        if not lbl_annotations['was_cancelled']:\n",
    "            # This if statement is meant to deal with issues that may arise with label studio if saving over a project\n",
    "            if len(lbl_annotations['result'])==0 and lbl_annotations.get('prediction'):\n",
    "                old_result = lbl_annotations['prediction']['result']\n",
    "                lbl_annotations['result'] = old_result if len(old_result) > 3 else lbl_annotations['result']\n",
    "            \n",
    "            # Convert label studio annotations\n",
    "            old_bbox_data = lbl_annotations['result']\n",
    "            # coco_bbox_data = []\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            for bbox in old_bbox_data:\n",
    "                old_bbox = bbox['value']\n",
    "                bbox = [old_bbox['x'], old_bbox['y'], old_bbox['x']+old_bbox['width'], old_bbox['y']+old_bbox['height']]\n",
    "                label = 1 if old_bbox['rectanglelabels'][0]=='bird' else 0\n",
    "                # coco_bbox_datum = {'bbox': coco_bbox, 'label': coco_label}\n",
    "                # coco_bbox_data.append(coco_bbox_datum)\n",
    "                boxes.append(bbox)\n",
    "                labels.append(label)\n",
    "            filename = os.path.basename(label['data']['image'])\n",
    "            if not any([file for file in excluded_images if file.split('.JPG')[0] in filename]):\n",
    "                annotations.append(filepath=str(Path(data_dir)/filename),\n",
    "                                   boxes=torch.tensor(boxes),\n",
    "                                   labels=torch.tensor(labels)\n",
    "                                  )\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing boxes around birds in holdout set: 100%|██████████| 2400/2400 [16:23<00:00,  2.44it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(params['models']['best_model'])\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "transform = get_transform()\n",
    "slices_holdout_dir = Path(params['slices']['holdouts_dir'])/params['data']['project_name']/'sliced_images'\n",
    "filenames = sorted(os.listdir(slices_holdout_dir))\n",
    "inferences = []\n",
    "\n",
    "for filename in tqdm(filenames, desc=\"Drawing boxes around birds in holdout set\"):\n",
    "    if not filename.startswith('.'):\n",
    "        img_id = '_'.join(filename.split('_')[:3]) + '.JPG'; # print(img_id)\n",
    "        raw_path = Path(params['data']['raw_dir'])/params['data']['project_name']/img_id; \n",
    "        regex = re.search(\"t(\\d{4})..(\\d{4})..(\\d{4})..(\\d{4})\", filename)\n",
    "        s_ids = regex.groups()\n",
    "        t,b,l,r = tuple(int(s_id) for s_id in s_ids)\n",
    "        raw_image = Image.open(raw_path);\n",
    "        raw_slice = raw_image.crop((l,t,r,b)); \n",
    "        raw_slice_nparray = cv2.cvtColor(np.array(raw_slice)[:,:,::-1].copy(), cv2.COLOR_BGR2RGB)\n",
    "        filepath = Path(slices_holdout_dir)/filename;\n",
    "        image = Image.open(filepath)\n",
    "        image = transform(image).unsqueeze(0).cuda() if torch.cuda.is_available() else transform(image).unsqueeze(0)\n",
    "        out = model(image)[0]\n",
    "        boxes = out['boxes'].cpu().detach().numpy()\n",
    "        scores = out['scores'].cpu().detach().numpy()\n",
    "        labels = out['labels'].cpu().detach().numpy()\n",
    "        inferences.append(dict(filepath=str(filepath),\n",
    "                                boxes=torch.tensor(boxes),\n",
    "                                scores=torch.tensor(scores),\n",
    "                                labels=torch.tensor(labels)\n",
    "                           ))\n",
    "        pred_boxes = out[\"boxes\"]\n",
    "        save_holdout_slices_with_inferences(params, filename, pred_boxes, raw_slice_nparray)\n",
    "save_inference_results(output_folder, inferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m     boxes\u001b[38;5;241m.\u001b[39mappend(bbox)\n\u001b[1;32m     34\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[0;32m---> 35\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(\u001b[43mlabel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m([file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m excluded_images \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.JPG\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m filename]):\n\u001b[1;32m     37\u001b[0m     annotations\u001b[38;5;241m.\u001b[39mappend(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(Path(data_dir)\u001b[38;5;241m/\u001b[39mfilename),\n\u001b[1;32m     38\u001b[0m                        boxes\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(boxes),\n\u001b[1;32m     39\u001b[0m                        labels\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(labels)\n\u001b[1;32m     40\u001b[0m                       )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "no_annotation_dir = Path(params['data']['holdouts_dir'])/(project_name+'_v1')\n",
    "no_annotation_filenames = sorted(os.listdir(no_annotation_dir))\n",
    "no_annotation = [filename for filename in no_annotation_filenames if filename.lower().endswith('.jpg')]\n",
    "filenames = sorted(os.listdir(slices_holdout_dir))\n",
    "\n",
    "# This labels variable is the filepath containing the output of the label-studio labelling job.\n",
    "labels_path = params['slices']['labels']\n",
    "# This function transforms, or re-formats, the labels into the required format for training.\n",
    "with open(labels_path, 'r') as fr:\n",
    "    # ground_truth = groundTruth_annotations(slices_holdout_dir, json.loads(fr.read()), no_annotation)\n",
    "    gt_annotation_pts = json.loads(fr.read())\n",
    "    excluded_images = no_annotation\n",
    "    annotations = []\n",
    "    for pts in gt_annotation_pts:\n",
    "        lbl_annotations = pts['annotations'][0]\n",
    "        if not lbl_annotations['was_cancelled']:\n",
    "            # This if statement is meant to deal with issues that may arise with label studio if saving over a project\n",
    "            if len(lbl_annotations['result'])==0 and lbl_annotations.get('prediction'):\n",
    "                old_result = lbl_annotations['prediction']['result']\n",
    "                lbl_annotations['result'] = old_result if len(old_result) > 3 else lbl_annotations['result']\n",
    "            \n",
    "            # Convert label studio annotations\n",
    "            old_bbox_data = lbl_annotations['result']\n",
    "            # coco_bbox_data = []\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            for bbox in old_bbox_data:\n",
    "                old_bbox = bbox['value']\n",
    "                bbox = [old_bbox['x'], old_bbox['y'], old_bbox['x']+old_bbox['width'], old_bbox['y']+old_bbox['height']]\n",
    "                label = 1 if old_bbox['rectanglelabels'][0]=='bird' else 0\n",
    "                # coco_bbox_datum = {'bbox': coco_bbox, 'label': coco_label}\n",
    "                # coco_bbox_data.append(coco_bbox_datum)\n",
    "                boxes.append(bbox)\n",
    "                labels.append(label)\n",
    "            filename = os.path.basename(pts['data']['image'])\n",
    "            if not any([file for file in excluded_images if file.split('.JPG')[0] in filename]):\n",
    "                annotations.append(filepath=str(Path(data_dir)/filename),\n",
    "                                   boxes=torch.tensor(boxes),\n",
    "                                   labels=torch.tensor(labels)\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(filenames[0:2], desc=\"Obtaining ground truth annotation for holdout set\"):\n",
    "    if filename in img_ids:\n",
    "        \n",
    "        boxes = out[0]['boxes'].cpu().detach().numpy()\n",
    "        labels = out[0]['labels'].cpu().detach().numpy()\n",
    "        ground_truth.append(dict(\n",
    "                                boxes=torch.tensor(boxes),\n",
    "                                labels=torch.tensor(labels),\n",
    "                              ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.27s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2022"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = BirdDataset(\n",
    "                        root=params['training']['train_data_dir'],\n",
    "                        annotation=params['training']['train_coco'],\n",
    "                        transforms=get_transform()\n",
    "                        )\n",
    "len(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': tensor(0.6139),\n",
      " 'map_50': tensor(1.),\n",
      " 'map_75': tensor(-1),\n",
      " 'map_large': tensor(0.6139),\n",
      " 'map_medium': tensor(-1.),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(-1.),\n",
      " 'mar_1': tensor(0.6139),\n",
      " 'mar_10': tensor(0.6139),\n",
      " 'mar_100': tensor(0.6139),\n",
      " 'mar_100_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.6139),\n",
      " 'mar_medium': tensor(-1.),\n",
      " 'mar_small': tensor(-1.)}\n"
     ]
    }
   ],
   "source": [
    "preds = [\n",
    "  dict(\n",
    "    boxes=torch.tensor([[258.0, 41.0, 606.0, 285.0]]),\n",
    "    scores=torch.tensor([0.536]),\n",
    "    labels=torch.tensor([1]),\n",
    "  )\n",
    "]\n",
    "target = [\n",
    "  dict(\n",
    "    boxes=torch.tensor([[214.0, 41.0, 562.0, 285.0]]),\n",
    "    labels=torch.tensor([1]),\n",
    "  )\n",
    "]\n",
    "metric = MeanAveragePrecision(box_format='xyxy', iou_type='bbox', iou_thresholds=list(np.linspace(0.5,0.95,101)))\n",
    "metric.update(preds, target)\n",
    "pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for key, value in predictions.items():\n",
    "    xs.append(ground_truth[key])\n",
    "    ys.append(value)\n",
    "\n",
    "xs = np.array(xs)\n",
    "ys = np.array(ys)\n",
    "\n",
    "xs_fit = xs[:,np.newaxis]\n",
    "a, res, _, _ = np.linalg.lstsq(xs_fit, ys, rcond=None)\n",
    "r2 = 1 - res / (ys.size * ys.var())\n",
    "\n",
    "# Output CSV\n",
    "output_basename = \"true_vs_predicted_counts\"\n",
    "dataset = pd.DataFrame({'MANUAL': xs, 'AUTOMATIC': ys}, columns=['MANUAL', 'AUTOMATIC'])\n",
    "dataset.to_csv(f\"{results_dir}/{output_basename}.csv\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "pylab.plot(xs,ys,'o')\n",
    "pylab.plot(xs,a*xs,\"r--\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8975dbe95a9696821a508f700b23ea164b3f8fcfb057d4e51a45b47dea22a8ca"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "waterbirds",
   "language": "python",
   "name": "waterbirds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
